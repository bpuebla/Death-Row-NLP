{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sTGfmlKURDEX",
        "zhz2nGSDjrwI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Death Row Last Statements analysis.\n",
        "\n",
        "We will go over last statements of executed Texas convicts, and analyse topics and try to infer the sentiment of each text."
      ],
      "metadata": {
        "id": "iIvru4rZQNRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install contractions\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "GNOTpfze_DOV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import warnings\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from lxml import etree\n",
        "import pandas as pd\n",
        "\n",
        "import spacy\n",
        "import contractions\n",
        "from gensim.models.phrases import Phraser\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_short, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, stem_text\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to assignment directory\n",
        "path_to_folder = '/content/drive/My Drive/slef'\n",
        "os.chdir(path_to_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j9FLgRYjU60",
        "outputId": "2a653abe-26a4-4295-cb21-b154ee550ada"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping and data loading\n",
        "We will scrape the webpage using BeautifulSoup, and we will compose a pandas Dataframe with last statements of each inmate. The code ignores inmates that did not provide a last statement."
      ],
      "metadata": {
        "id": "rgRcO7QgQmHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read HTML page using requests\n",
        "url = 'https://www.tdcj.texas.gov/death_row/dr_executed_offenders.html'\n",
        "html = requests.get(url,verify=False).content\n",
        "\n",
        "# Parse HTML using Beautiful Soup\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "dom =  etree.HTML(str(soup))\n",
        "last_stmt_link = dom.xpath('//a[contains(text(), \"Last Statement\")]')\n",
        "\n",
        "output = []\n",
        "\n",
        "for elem in last_stmt_link:\n",
        "    try:\n",
        "        link = elem.get('href')\n",
        "        # Read HTML page for current prisoner's last statement\n",
        "        html = requests.get(f'https://www.tdcj.texas.gov/death_row/{link}',verify=False).content\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        # Get prisoner's last statement text\n",
        "        text = soup.select('p:nth-child(11)')[0].get_text().strip()\n",
        "        # Assign 'None' if prisoner said nothing\n",
        "        if text != '' and text != 'No last statement given.':\n",
        "            output.append(text)\n",
        "    except:\n",
        "        print(f\"page {link} not found\")\n",
        "        text = 'None'"
      ],
      "metadata": {
        "id": "WWFwfByNJGak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06828122-34b6-41c0-fc1d-12fcb418f09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page dr_info/greengarylast.html not found\n",
            "page dr_info/runnelstravislast.html not found\n",
            "page dr_info/youngchristopherlast.html not found\n",
            "page dr_info/bibledannylast.html not found\n",
            "page /death_row/dr_info/cardenasrubenlast.html not found\n",
            "page /death_row/dr_info/pruettrobertlast.html not found\n",
            "page dr_info/vasquezpablolast.html not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "statement_df = pd.DataFrame(output, columns=['Last Statement'])"
      ],
      "metadata": {
        "id": "GqLwtWHaVLzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statement_df.to_csv('data.csv')"
      ],
      "metadata": {
        "id": "b8VzCdxlVYzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving to csv and loading."
      ],
      "metadata": {
        "id": "X0m154jmQ_eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')"
      ],
      "metadata": {
        "id": "4fHL1jPdjLiA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[df['Last Statement'] != 'No last statement given.']"
      ],
      "metadata": {
        "id": "y47Y_3vYlE0_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOGqNjQslviP",
        "outputId": "59400d82-7772-4449-d97c-df1e3f50cea2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(475, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "sTGfmlKURDEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus.reader.wordnet import VERB, NOUN, ADJ, ADV\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "FILTERS=[lambda x: x.lower(), strip_tags, strip_punctuation,\n",
        "    strip_multiple_whitespaces, strip_numeric,\n",
        "    remove_stopwords, strip_short]\n",
        "py_lemmatizer = WordNetLemmatizer()\n",
        "dict_pos_map = {\n",
        "    # Look for NN in the POS tag because all nouns begin with NN\n",
        "    'NN': NOUN,\n",
        "    # Look for VB in the POS tag because all nouns begin with VB\n",
        "    'VB':VERB,\n",
        "    # Look for JJ in the POS tag because all nouns begin with JJ\n",
        "    'JJ' : ADJ,\n",
        "    # Look for RB in the POS tag because all nouns begin with RB\n",
        "    'RB':ADV,\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuLQ40WDTmzz",
        "outputId": "94e9ca0c-e562-4bb9-8045-16ecdc0bebd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_names(text):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    clean_text = \"\"\n",
        "\n",
        "    # Iterate over each sentence\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence into words\n",
        "        words = word_tokenize(sentence)\n",
        "\n",
        "        words = [contractions.fix(word) for word in words]\n",
        "\n",
        "        # Tag the words with their parts of speech\n",
        "        tagged_words = pos_tag(words)\n",
        "\n",
        "        # Filter out proper nouns (tags starting with 'NNP')\n",
        "        filtered_words = [py_lemmatizer.lemmatize(word,dict_pos_map[tag[:2]]) for word, tag in tagged_words if (tag[:2] in dict_pos_map.keys() and tag !='NNP')]\n",
        "\n",
        "        # Join the words back into a sentence\n",
        "        cleaned_sentence = \" \".join(filtered_words)\n",
        "\n",
        "        # Append the cleaned sentence to the final text\n",
        "        clean_text += cleaned_sentence + \" \"\n",
        "\n",
        "    clean_text = preprocess_string(clean_text, FILTERS)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "XuD9ggFMTv6F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tokenized'] = df['Last Statement'].apply(lambda doc: remove_names(doc)) # Lemmatizes"
      ],
      "metadata": {
        "id": "ajxAAVT5W1Dj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"------ Before Text Preprocessing ------\\n\")\n",
        "print(df['Last Statement'][9])\n",
        "print('')\n",
        "print(\"------ After Text Preprocessing ------\\n\")\n",
        "print(df['Tokenized'][9])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agvkl8OYXclO",
        "outputId": "28034d4d-1472-48fb-dc61-ae4eb572841a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Before Text Preprocessing ------\n",
            "\n",
            "No statement was made.\n",
            "\n",
            "------ After Text Preprocessing ------\n",
            "\n",
            "['statement']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "BslG-fItgv_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "model=api.load(\"word2vec-google-news-300\")\n",
        "vec_size=300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la-F0ckhgyv_",
        "outputId": "af37fa4e-aac5-458d-9b30-08d0043515e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 99.9% 1660.4/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "from gensim.models import Word2Vec\n",
        "vec_size=300\n",
        "w2v_model = Word2Vec(df['Tokenized'].tolist(),\n",
        "                     min_count=3,\n",
        "                     window=4,\n",
        "                     vector_size=vec_size,\n",
        "                     sample=1e-5,\n",
        "                     alpha=0.03,\n",
        "                     min_alpha=0.0007,\n",
        "                     negative=20,\n",
        "                     workers=multiprocessing.cpu_count()-1)\n",
        "model = w2v_model.wv"
      ],
      "metadata": {
        "id": "eF8Lv8rRpknO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "words = set(model.index_to_key )\n",
        "X_vect = np.array([np.array([model[i] for i in ls if i in words]) # Get embeddings for each word\n",
        "                         for ls in df['Tokenized'].tolist()])\n",
        "X_vect.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ff3TCgkB-x",
        "outputId": "b8e78cf7-f38d-40c7-884d-b1247a325543"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(475,)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vect_avg = []\n",
        "for v in X_vect:\n",
        "    if v.size:\n",
        "        X_train_vect_avg.append(v.mean(axis=0)) # Average across each embedding dimension\n",
        "    else:\n",
        "        X_train_vect_avg.append(np.zeros(vec_size, dtype=float)) # If size is zero, just append zeroes"
      ],
      "metadata": {
        "id": "cT-GitSYpF9a"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "km = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=X_train_vect_avg)\n",
        "positive_cluster_center = km.cluster_centers_[0]\n",
        "negative_cluster_center = km.cluster_centers_[1]"
      ],
      "metadata": {
        "id": "t3ueaCIYkTBN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.similar_by_vector(km.cluster_centers_[0], topn=10, restrict_vocab=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9hfkQ9SpNR4",
        "outputId": "80192117-8eb3-4314-d1e4-d7bbb3350b08"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('love', 0.9849041700363159),\n",
              " ('people', 0.9407002329826355),\n",
              " ('know', 0.9296285510063171),\n",
              " ('life', 0.9207703471183777),\n",
              " ('come', 0.9147742390632629),\n",
              " ('want', 0.9036694169044495),\n",
              " ('forgive', 0.8843216896057129),\n",
              " ('family', 0.8750388622283936),\n",
              " ('thing', 0.8601243495941162),\n",
              " ('continue', 0.8590933680534363)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.similar_by_vector(km.cluster_centers_[1], topn=10, restrict_vocab=None) # basura"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEBsONmrpZzW",
        "outputId": "5f4f133f-3382-4325-bf0d-b3bb54110109"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('love', 0.9992219805717468),\n",
              " ('people', 0.9202014207839966),\n",
              " ('know', 0.9010292887687683),\n",
              " ('life', 0.8999360799789429),\n",
              " ('come', 0.8968741297721863),\n",
              " ('want', 0.8849157691001892),\n",
              " ('forgive', 0.8675839900970459),\n",
              " ('family', 0.8430299162864685),\n",
              " ('thing', 0.8412821292877197),\n",
              " ('continue', 0.8402407765388489)]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modeling"
      ],
      "metadata": {
        "id": "zhz2nGSDjrwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Inutil completamente. Borrar.\n",
        "corpus=df.Tokenized.values.tolist()\n",
        "from gensim.corpora import Dictionary #Dictionary creation\n",
        "\n",
        "D = Dictionary(corpus)\n",
        "n_tokens = len(D)\n",
        "no_below = 4\n",
        "no_above = 0.8\n",
        "\n",
        "D.filter_extremes(no_below=no_below,no_above=no_above)#Filtering\n",
        "n_tokens = len(D)\n",
        "\n",
        "print('The dictionary contains', n_tokens, 'terms')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhw49pObPKf-",
        "outputId": "c80a7c91-2732-47ac-cfa7-a973a593106b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dictionary contains 421 terms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions_bow = [D.doc2bow(doc) for doc in corpus] #BOW\n",
        "\n",
        "n_question = 10\n",
        "print(' '.join(corpus[n_question]))\n",
        "\n",
        "print(questions_bow[n_question])\n",
        "\n",
        "print(list(map(lambda x: (D[x[0]], x[1]), questions_bow[n_question])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KMA1Sy1T0Xj",
        "outputId": "c108a264-44e5-4a4f-8b37-5adced4fd26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lay sleep die justice soul wake truly regret kill family thankful thought prayer family day love\n",
            "[(10, 1), (11, 1), (35, 2), (42, 1), (73, 1), (74, 1), (100, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1)]\n",
            "[('justice', 1), ('kill', 1), ('family', 2), ('love', 1), ('day', 1), ('die', 1), ('regret', 1), ('lay', 1), ('prayer', 1), ('sleep', 1), ('soul', 1), ('thankful', 1), ('truly', 1), ('wake', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.ldamodel import LdaModel\n",
        "num_topics = 20\n",
        "\n",
        "ldag = LdaModel(corpus=questions_bow, id2word=D, num_topics=num_topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQzZQD-lT8nj",
        "outputId": "e55e2781-5433-4127-e626-1ee741e106ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        }
      ]
    }
  ]
}